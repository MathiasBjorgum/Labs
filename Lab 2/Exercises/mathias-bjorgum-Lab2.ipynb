{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 - Tagging\n",
    "\n",
    "TDT4310, Spring 2022\\\n",
    "Lab date: February 1, 2022\\\n",
    "Prof. Björn Gambäck\\\n",
    "TA. Tollef Jørgensen\n",
    "\n",
    "Student: Mathias Bjørgum, mathigb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "This lab uses ``python 3.10.2`` running  in an ``conda`` environment with `nltk version: 3.6.7`. The following modules will be imported and used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, OrderedDict\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "from nltk.tag import DefaultTagger, UnigramTagger, RegexpTagger, BigramTagger, TrigramTagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### 1 - Ambiguity\n",
    "\n",
    "Use the *Brown Corpus* and...\n",
    "\n",
    "#### a) Print the 5 most frequent tags in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NN', 152470), ('IN', 120557), ('AT', 97959), ('JJ', 64028), ('.', 60638)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_corpus = nltk.corpus.brown\n",
    "\n",
    "nltk.FreqDist([tag for word, tag in brown_corpus.tagged_words()]).most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 5 most common tags:\n",
    "\n",
    "`[('NN', 152470), ('IN', 120557), ('AT', 97959), ('JJ', 64028), ('.', 60638)]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) How many words are ambiguous, in the sense that they appear with more than two tags?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1543\n"
     ]
    }
   ],
   "source": [
    "tags = defaultdict(set)\n",
    "\n",
    "# Each word gets its own key in a dictionary\n",
    "for word, tag in brown_corpus.tagged_words():\n",
    "    tags[word].add(tag)\n",
    "\n",
    "count = 0\n",
    "\n",
    "# Counts all words that has multiple tags\n",
    "for word, tag_list in tags.items():\n",
    "    if len(tag_list) > 2:\n",
    "        count +=1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of words that have multiple tags are 1543."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Print the percentage of ambiguous words in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1543 / 56057 = 0.027525554346468774\n"
     ]
    }
   ],
   "source": [
    "# Creates a set of all the words\n",
    "words = set([word for word, tag in brown_corpus.tagged_words()])\n",
    "\n",
    "print(f\"{count} / {len(words)} = {count / len(words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the percentage of words that are ambigous are 2.753%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Find the top 5 words (longer than 4 characters) with the highest number of distinct tags. Select one of them and print out a sentence with the word in its different forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Little', {'JJ-HL', 'JJ', 'AP-HL', 'QL', 'NP', 'JJ-TL', 'AP'}), ('Chinese', {'JJ-HL', 'NP-NC', 'JJ-NC', 'JJ', 'NP', 'NPS', 'JJ-TL'}), ('still', {'JJ', 'NN', 'QL', 'VB', 'QLP', 'RB'}), ('right', {'NN', 'JJ', 'NN-HL', 'QL', 'NR', 'RB'}), ('Light', {'JJ-HL', 'NN', 'JJ', 'NN-TL', 'VB', 'JJ-TL'})]\n"
     ]
    }
   ],
   "source": [
    "words_longer_than_four = [(str(word), tag_list) for word, tag_list in tags.items() if len(str(word)) > 4]\n",
    "ordered_based_on_value_length = OrderedDict(sorted(words_longer_than_four, key=lambda item: len(item[1]), reverse=True))\n",
    "\n",
    "top_five = [ordered_based_on_value_length.popitem(False) for i in range(5)]\n",
    "\n",
    "print(top_five)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the top five words with the highest number of disinct tags are:\n",
    "\n",
    "`[('Little', {'AP-HL', 'AP', 'NP', 'QL', 'JJ-HL', 'JJ-TL', 'JJ'}), ('Chinese', {'JJ-NC', 'NPS', 'NP-NC', 'NP', 'JJ-HL', 'JJ-TL', 'JJ'}), ('still', {'RB', 'QL', 'VB', 'QLP', 'NN', 'JJ'}), ('right', {'NN-HL', 'NR', 'RB', 'QL', 'NN', 'JJ'}), ('Light', {'JJ-HL', 'VB', 'JJ-TL', 'NN', 'JJ', 'NN-TL'})]`\n",
    "\n",
    "I select the word *\"still\"* to find sentences with the words in different forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_one_sentence(target_word: str, target_tag: str):\n",
    "    for sent_tags in brown_corpus.tagged_sents():\n",
    "        for word, word_tag in sent_tags:\n",
    "            if word == target_word and word_tag == tag:\n",
    "                return sent_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_word = \"still\"\n",
    "target_tags = tags[target_word]\n",
    "sents = []\n",
    "\n",
    "for tag in target_tags:\n",
    "    sents.append([find_one_sentence(target_word, tag), tag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Similarly , in presenting still photographs of early jazz groups , the program allowed no time for a close perusal . ',\n",
       "  'JJ'],\n",
       " [\"I aim to keep a little whisky still back in the ridge for my pleasure '' . \",\n",
       "  'NN'],\n",
       " ['Further improvements in earnings of the Kansas Turnpike are expected late in 1961 , with the opening of a new bypass at Wichita , and still later when the turnpike gets downtown connections in both Kansas City , Kans. , and Kansas City , Mo. . ',\n",
       "  'QL'],\n",
       " ['Recent statements by well-known scientists regarding the destructive power of the newest nuclear bombs and the deadly fall-outs should be sufficient to still the voices of those who advocate nuclear warfare instead of negotiations . ',\n",
       "  'VB'],\n",
       " ['In the future , quantitative demand will be greater because of the expansion of the economy , and the qualitative need will be greater still . ',\n",
       "  'QLP'],\n",
       " [\"While details are still to be worked out , Ratcliff said he expects to tell home folks in Dallas why he thinks Berry's proposed constitutional amendment should be rejected . \",\n",
       "  'RB']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "untagged_sents = []\n",
    "\n",
    "for tagged_sent in sents:\n",
    "    sent = \"\"\n",
    "    for word, tag in tagged_sent[0]:\n",
    "        sent += word\n",
    "        sent += \" \"\n",
    "    \n",
    "    untagged_sents.append([sent, tagged_sent[1]])\n",
    "\n",
    "(untagged_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us six sentences where *\"still\"* has different roles. The sentences are as follows:\n",
    "\n",
    "```\n",
    "\"While details are still to be worked out , Ratcliff said he expects to tell home folks in Dallas why he thinks Berry's proposed constitutional amendment should be rejected . \", role:  'RB'\n",
    "\n",
    "'Further improvements in earnings of the Kansas Turnpike are expected late in 1961 , with the opening of a new bypass at Wichita , and still later when the turnpike gets downtown connections in both Kansas City , Kans. , and Kansas City , Mo. . ',role:  'QL'\n",
    "\n",
    "'Recent statements by well-known scientists regarding the destructive power of the newest nuclear bombs and the deadly fall-outs should be sufficient to still the voices of those who advocate nuclear warfare instead of negotiations . ', role:  'VB'\n",
    "\n",
    "'In the future , quantitative demand will be greater because of the expansion of the economy , and the qualitative need will be greater still . ', role:  'QLP'\n",
    "\n",
    "\"I aim to keep a little whisky still back in the ridge for my pleasure '' . \", role:  'NN'\n",
    "\n",
    "'Similarly , in presenting still photographs of early jazz groups , the program allowed no time for a close perusal . ', role:  'JJ'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Training a tagger\n",
    "\n",
    "Explore the performance of a tagger using the Brown Corpus and NPS Chat Corpus as data sources, with\n",
    "different ratios of train/test data. Use the following ratios:\n",
    "\n",
    "* Brown 90% / NPS 10%\n",
    "* Brown 50% /NPS 50%\n",
    "* NPS 90% / Brown 10%\n",
    "* NPS 50% / Brown 50%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_corpus = nltk.corpus.brown\n",
    "NPS_corpus = nltk.corpus.nps_chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I define the train/test splits, and adds them to a list to use later. I choose to use the universal tagset in this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_permuatations(brown_corpus, NPS_corpus, tagset_universal = True):\n",
    "    random_state = 4310\n",
    "    if not tagset_universal:\n",
    "        brown_train_90, brown_test_10 = split(brown_corpus.tagged_sents(), train_size=0.9, random_state=random_state)\n",
    "        brown_train_50, brown_test_50 = split(brown_corpus.tagged_sents(), train_size=0.5, random_state=random_state)\n",
    "        NPS_train_90, NPS_test_10 = split(NPS_corpus.tagged_posts(), train_size=0.9, random_state=random_state)\n",
    "        NPS_train_50, NPS_test_50 = split(NPS_corpus.tagged_posts(), train_size=0.5, random_state=random_state)\n",
    "\n",
    "    if tagset_universal:\n",
    "        brown_train_90, brown_test_10 = split(brown_corpus.tagged_sents(tagset=\"universal\"), train_size=0.9, random_state=random_state)\n",
    "        brown_train_50, brown_test_50 = split(brown_corpus.tagged_sents(tagset=\"universal\"), train_size=0.5, random_state=random_state)\n",
    "        NPS_train_90, NPS_test_10 = split(NPS_corpus.tagged_posts(tagset=\"universal\"), train_size=0.9, random_state=random_state)\n",
    "        NPS_train_50, NPS_test_50 = split(NPS_corpus.tagged_posts(tagset=\"universal\"), train_size=0.5, random_state=random_state)\n",
    "    \n",
    "    train_test_permutations = [(brown_train_90, NPS_test_10),\n",
    "        (brown_train_50, NPS_test_50),\n",
    "        (NPS_train_90, brown_test_10),\n",
    "        (NPS_train_50, brown_test_50)]\n",
    "    return train_test_permutations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_permutations = get_train_test_permuatations(\n",
    "    brown_corpus, NPS_corpus, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "def find_most_common_tag(dataset: List[tuple[str, str]]) -> str:\n",
    "    '''Assumes that the input set is a list of tagged words. Returns the most common tag.'''\n",
    "    tags = []\n",
    "    for sent in dataset:\n",
    "        for word, tag in sent:\n",
    "            tags.append(tag)\n",
    "\n",
    "    return nltk.FreqDist(tags).most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_tagger_from_train_split(train_split):\n",
    "    return DefaultTagger(find_most_common_tag(train_split))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Create a *DefaultTagger* using the most common tag in each corpus as the default tag\n",
    "\n",
    "I assume that we are supposed to create a new *DefaultTagger* for each permutation of train/test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_taggers = [get_default_tagger_from_train_split(train_split) for train_split, test_split in train_test_permutations]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Create a combined tagger with the RegEx tagger (see Ch. 5, sec. 4.2) with an initial backoff using the most common default tag. Then, use n-gram taggers as backoff taggers (e.g., UnigramTagger, BigramTagger, TrigramTagger). The ordering is up to you, but justify your choice. Calculate the accuracy of each of the four train/test permutations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_accuracy_of_taggers(taggers: List, train_test_permutations: List) -> None:\n",
    "    '''\n",
    "    Prints the accuracy of each tagger and its corresponding test split.\n",
    "\n",
    "    If the lenght of taggers is equal to 1, it will only use the one tagger.\n",
    "    '''\n",
    "\n",
    "    for i, permutation in enumerate(train_test_permutations):\n",
    "        permutation_no = i\n",
    "        if len(taggers) == 1: i = 0\n",
    "        print(f\"Accuracy of permuatation {permutation_no+1}: {taggers[i].accuracy(permutation[1]):2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combined_reg_ex_tagger(train_split, n_gram: int = 0, tagset_universal: bool = True):\n",
    "    '''\n",
    "    Takes a training dataset `train_split` and a number of n-gram `n_gram` to create a `RegexpTagger`.\n",
    "    \n",
    "    `n_gram` defaults to zero, i.e a default tagger. `tagset_universal` defaults to True.\n",
    "    '''\n",
    "    if tagset_universal:\n",
    "        patterns = [\n",
    "        (r'.*ing$', 'VERB'), # gerunds\n",
    "        (r'.*ed$', 'VERB'), # simple past\n",
    "        (r'.*es$', 'VERB'), # 3rd singular present\n",
    "        (r'.*ould$', 'VERB'), # modals\n",
    "        (r'.*\\'s$', 'NOUN'), # possessive nouns\n",
    "        (r'.*s$', 'NOUN'), # plural nouns\n",
    "        (r'^-?[0-9]+(\\.[0-9]+)?$', 'NUM'), # cardinal numbers\n",
    "        ]\n",
    "    if not tagset_universal:\n",
    "        patterns = [\n",
    "        (r'.*ing$', 'VBG'), # gerunds\n",
    "        (r'.*ed$', 'VBD'), # simple past\n",
    "        (r'.*es$', 'VBZ'), # 3rd singular present\n",
    "        (r'.*ould$', 'MD'), # modals\n",
    "        (r'.*\\'s$', 'NN$'), # possessive nouns\n",
    "        (r'.*s$', 'NNS'), # plural nouns\n",
    "        (r'^-?[0-9]+(\\.[0-9]+)?$', 'CD'), # cardinal numbers\n",
    "        ]\n",
    "\n",
    "    t0 = get_default_tagger_from_train_split(train_split)\n",
    "    t1 = UnigramTagger(train_split, backoff=t0)\n",
    "    t2 = BigramTagger(train_split, backoff=t1)\n",
    "    t3 = TrigramTagger(train_split, backoff=t2)\n",
    "\n",
    "    if n_gram == 1:\n",
    "        return RegexpTagger(patterns, backoff = t1)\n",
    "    if n_gram == 2:\n",
    "        return RegexpTagger(patterns, backoff = t2)\n",
    "    if n_gram == 3:\n",
    "        return RegexpTagger(patterns, backoff = t3)\n",
    "\n",
    "    return RegexpTagger(patterns, backoff = t0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-gram tagger = 0\n",
      "Accuracy of permuatation 1: 0.231729\n",
      "Accuracy of permuatation 2: 0.239749\n",
      "Accuracy of permuatation 3: 0.280140\n",
      "Accuracy of permuatation 4: 0.279242\n",
      "N-gram tagger = 1\n",
      "Accuracy of permuatation 1: 0.602626\n",
      "Accuracy of permuatation 2: 0.598718\n",
      "Accuracy of permuatation 3: 0.784156\n",
      "Accuracy of permuatation 4: 0.774193\n",
      "N-gram tagger = 2\n",
      "Accuracy of permuatation 1: 0.602845\n",
      "Accuracy of permuatation 2: 0.598805\n",
      "Accuracy of permuatation 3: 0.784019\n",
      "Accuracy of permuatation 4: 0.772226\n",
      "N-gram tagger = 3\n",
      "Accuracy of permuatation 1: 0.600656\n",
      "Accuracy of permuatation 2: 0.596449\n",
      "Accuracy of permuatation 3: 0.777352\n",
      "Accuracy of permuatation 4: 0.758848\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(f\"N-gram tagger = {i}\")\n",
    "    print_accuracy_of_taggers(\n",
    "        taggers = [get_combined_reg_ex_tagger(split[0], n_gram = i, tagset_universal=True)\n",
    "        for split in train_test_permutations],\n",
    "        train_test_permutations=train_test_permutations\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the accuracy of all the permuatations. I get the following output:\n",
    "```\n",
    "N-gram tagger = 0\n",
    "Accuracy of permuatation 1: 0.231729\n",
    "Accuracy of permuatation 2: 0.239749\n",
    "Accuracy of permuatation 3: 0.280140\n",
    "Accuracy of permuatation 4: 0.279242\n",
    "\n",
    "N-gram tagger = 1\n",
    "Accuracy of permuatation 1: 0.602626\n",
    "Accuracy of permuatation 2: 0.598718\n",
    "Accuracy of permuatation 3: 0.784156\n",
    "Accuracy of permuatation 4: 0.774193\n",
    "\n",
    "N-gram tagger = 2\n",
    "Accuracy of permuatation 1: 0.602845\n",
    "Accuracy of permuatation 2: 0.598805\n",
    "Accuracy of permuatation 3: 0.784019\n",
    "Accuracy of permuatation 4: 0.772226\n",
    "\n",
    "N-gram tagger = 3\n",
    "Accuracy of permuatation 1: 0.600656\n",
    "Accuracy of permuatation 2: 0.596449\n",
    "Accuracy of permuatation 3: 0.777352\n",
    "Accuracy of permuatation 4: 0.758848\n",
    "```\n",
    "\n",
    "**NOTE**: This takes around 2m to run, and i am not quite sure why.\n",
    "\n",
    "Based on this it looks like training on the **NPS dataset** and testing on the **Brown dataset** gived the best accuracy. We can see that the accuracy increases drastically from default tagger to a unigram tagger, but after that it goes down. It looks like the Unigram tagger gives the best result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Select a dataset split of your choice and print a table containing the precision, recall and f-measure for the top 5 most common tags (look up truncate in the documentation) and sort each score by count. Do this for all your chosen variations of backoffs (e.g., DefaultTagger, UnigramTagger and BigramTagger).\n",
    "\n",
    "I choose dataset split no. 3, since it gave the best performance in the previous task. I also strip the tags of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NOUN', 8695), ('VERB', 8077), ('X', 5901), ('PRON', 4202), ('.', 3828)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = train_test_permutations[2]\n",
    "taggers = [get_combined_reg_ex_tagger(dataset[0], n_gram = i) for i in range(4)]\n",
    "\n",
    "tags = []\n",
    "for post in dataset[0]:\n",
    "    for word, tag in post:\n",
    "        tags.append(tag)\n",
    "\n",
    "top_five_tags = nltk.FreqDist(tags).most_common(5)\n",
    "top_five_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def strip_tag_or_word(tagged_sents: List[List[Tuple[str, str]]], strip_words: bool = False):\n",
    "    '''Strips words or tags of tagged sentences depending on a boolean. `False` = strip tag'''\n",
    "    stripped_sents = []\n",
    "    for sent in tagged_sents:\n",
    "        sentence = []\n",
    "        for word, tag in sent:\n",
    "            if strip_words:\n",
    "                sentence.append(tag)\n",
    "            else:\n",
    "                sentence.append(word)\n",
    "        stripped_sents.append(sentence)\n",
    "\n",
    "    return stripped_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_no_labels = strip_tag_or_word(dataset[1], False)\n",
    "\n",
    "test_tags = [taggers[i].tag_sents(test_set_no_labels) for i in range(len(taggers))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-gram tagger: 0\n",
      " Tag | Prec.  | Recall | F-measure\n",
      "-----+--------+--------+-----------\n",
      "NOUN | 0.4435 | 0.9101 | 0.5964\n",
      "VERB | 0.7293 | 0.3677 | 0.4889\n",
      "   . | 0.0000 | 0.0000 | 0.0000\n",
      "PRON | 0.0000 | 0.0000 | 0.0000\n",
      "   X | 0.0000 | 0.0000 | 0.0000\n",
      "\n",
      "N-gram tagger: 1\n",
      " Tag | Prec.  | Recall | F-measure\n",
      "-----+--------+--------+-----------\n",
      "NOUN | 0.8047 | 0.8812 | 0.8413\n",
      "VERB | 0.8127 | 0.7397 | 0.7745\n",
      "   . | 0.9999 | 0.9412 | 0.9696\n",
      "PRON | 0.9983 | 0.9724 | 0.9852\n",
      "   X | 0.0523 | 0.0692 | 0.0596\n",
      "\n",
      "N-gram tagger: 2\n",
      " Tag | Prec.  | Recall | F-measure\n",
      "-----+--------+--------+-----------\n",
      "NOUN | 0.8057 | 0.8866 | 0.8442\n",
      "VERB | 0.8150 | 0.7380 | 0.7746\n",
      "   . | 0.9999 | 0.9409 | 0.9695\n",
      "PRON | 0.9900 | 0.9724 | 0.9811\n",
      "   X | 0.1125 | 0.0692 | 0.0857\n",
      "\n",
      "N-gram tagger: 3\n",
      " Tag | Prec.  | Recall | F-measure\n",
      "-----+--------+--------+-----------\n",
      "NOUN | 0.8051 | 0.8867 | 0.8439\n",
      "VERB | 0.8151 | 0.7377 | 0.7745\n",
      "   . | 0.9999 | 0.9248 | 0.9609\n",
      "PRON | 0.9899 | 0.9680 | 0.9788\n",
      "   X | 0.0281 | 0.0692 | 0.0400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics import ConfusionMatrix\n",
    "\n",
    "ref = [x for l in strip_tag_or_word(dataset[1], True) for x in l]\n",
    "for i, test_set in enumerate(test_tags):\n",
    "    tst = [x for l in strip_tag_or_word(test_set, True) for x in l]\n",
    "    cm = ConfusionMatrix(ref,tst)\n",
    "    # Maybe a bit cheesy to do it this way, but it is the best way i can think of.\n",
    "    cm._values = [tag for tag, _ in top_five_tags]\n",
    "    print(f\"N-gram tagger: {i}\")\n",
    "    print(cm.evaluate(sort_by_count=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this i get this output:\n",
    "\n",
    "```\n",
    "N-gram tagger: 0\n",
    " Tag | Prec.  | Recall | F-measure\n",
    "-----+--------+--------+-----------\n",
    "NOUN | 0.4435 | 0.9101 | 0.5964\n",
    "VERB | 0.7293 | 0.3677 | 0.4889\n",
    "   . | 0.0000 | 0.0000 | 0.0000\n",
    "PRON | 0.0000 | 0.0000 | 0.0000\n",
    "   X | 0.0000 | 0.0000 | 0.0000\n",
    "\n",
    "N-gram tagger: 1\n",
    " Tag | Prec.  | Recall | F-measure\n",
    "-----+--------+--------+-----------\n",
    "NOUN | 0.8047 | 0.8812 | 0.8413\n",
    "VERB | 0.8127 | 0.7397 | 0.7745\n",
    "   . | 0.9999 | 0.9412 | 0.9696\n",
    "PRON | 0.9983 | 0.9724 | 0.9852\n",
    "   X | 0.0523 | 0.0692 | 0.0596\n",
    "\n",
    "N-gram tagger: 2\n",
    " Tag | Prec.  | Recall | F-measure\n",
    "-----+--------+--------+-----------\n",
    "NOUN | 0.8057 | 0.8866 | 0.8442\n",
    "VERB | 0.8150 | 0.7380 | 0.7746\n",
    "   . | 0.9999 | 0.9409 | 0.9695\n",
    "PRON | 0.9900 | 0.9724 | 0.9811\n",
    "   X | 0.1125 | 0.0692 | 0.0857\n",
    "\n",
    "N-gram tagger: 3\n",
    " Tag | Prec.  | Recall | F-measure\n",
    "-----+--------+--------+-----------\n",
    "NOUN | 0.8051 | 0.8867 | 0.8439\n",
    "VERB | 0.8151 | 0.7377 | 0.7745\n",
    "   . | 0.9999 | 0.9248 | 0.9609\n",
    "PRON | 0.9899 | 0.9680 | 0.9788\n",
    "   X | 0.0281 | 0.0692 | 0.0400\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Using the *Brown Coprus*, create a baseline tagger (e.g Unigram) with a lookup model (see Ch. 5, sec. 4.3). The model should handle the most 200 common words and store the tags. Evaluate the accuracy on the above permutations of train/test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = nltk.FreqDist(brown_corpus.words())\n",
    "cfd = nltk.ConditionalFreqDist(brown_corpus.tagged_words(tagset=\"universal\"))\n",
    "most_common_200 = fd.most_common(200)\n",
    "likely_tags = dict((word, cfd[word].max()) for (word, _) in most_common_200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of permuatation 1: 0.321007\n",
      "Accuracy of permuatation 2: 0.319883\n",
      "Accuracy of permuatation 3: 0.547006\n",
      "Accuracy of permuatation 4: 0.547247\n"
     ]
    }
   ],
   "source": [
    "baseline_tagger = nltk.UnigramTagger(model = likely_tags, \n",
    "    backoff=get_default_tagger_from_train_split(brown_corpus.tagged_sents()))\n",
    "    \n",
    "print_accuracy_of_taggers([baseline_tagger], \n",
    "    train_test_permutations=train_test_permutations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not suprisingly this model performs significantly better when testing on the *Brown* dataset, since that is the one it has been trained on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e) With an arbitrary text from another corpus (or an article you scraped in Lab 1), use the tagger you just created and print a few tagged sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence 1\n",
      "[('30', 'NN'), (':', '.'), ('22', 'NN'), ('Moreover', 'NN'), ('the', 'DET'), ('LORD', 'NN'), ('spake', 'NN'), ('unto', 'NN'), ('Moses', 'NN'), (',', '.'), ('saying', 'NN'), (',', '.'), ('30', 'NN'), (':', '.'), ('23', 'NN'), ('Take', 'NN'), ('thou', 'NN'), ('also', 'ADV'), ('unto', 'NN'), ('thee', 'NN'), ('principal', 'NN'), ('spices', 'NN'), (',', '.'), ('of', 'ADP'), ('pure', 'NN'), ('myrrh', 'NN'), ('five', 'NN'), ('hundred', 'NN'), ('shekels', 'NN'), (',', '.'), ('and', 'CONJ'), ('of', 'ADP'), ('sweet', 'NN'), ('cinnamon', 'NN'), ('half', 'NN'), ('so', 'ADV'), ('much', 'ADJ'), (',', '.'), ('even', 'ADV'), ('two', 'NUM'), ('hundred', 'NN'), ('and', 'CONJ'), ('fifty', 'NN'), ('shekels', 'NN'), (',', '.'), ('and', 'CONJ'), ('of', 'ADP'), ('sweet', 'NN'), ('calamus', 'NN'), ('two', 'NUM'), ('hundred', 'NN'), ('and', 'CONJ'), ('fifty', 'NN'), ('shekels', 'NN'), (',', '.'), ('30', 'NN'), (':', '.'), ('24', 'NN'), ('And', 'CONJ'), ('of', 'ADP'), ('cassia', 'NN'), ('five', 'NN'), ('hundred', 'NN'), ('shekels', 'NN'), (',', '.'), ('after', 'ADP'), ('the', 'DET'), ('shekel', 'NN'), ('of', 'ADP'), ('the', 'DET'), ('sanctuary', 'NN'), (',', '.'), ('and', 'CONJ'), ('of', 'ADP'), ('oil', 'NN'), ('olive', 'NN'), ('an', 'DET'), ('hin', 'NN'), (':', '.'), ('30', 'NN'), (':', '.'), ('25', 'NN'), ('And', 'CONJ'), ('thou', 'NN'), ('shalt', 'NN'), ('make', 'VERB'), ('it', 'PRON'), ('an', 'DET'), ('oil', 'NN'), ('of', 'ADP'), ('holy', 'NN'), ('ointment', 'NN'), (',', '.'), ('an', 'DET'), ('ointment', 'NN'), ('compound', 'NN'), ('after', 'ADP'), ('the', 'DET'), ('art', 'NN'), ('of', 'ADP'), ('the', 'DET'), ('apothecary', 'NN'), (':', '.'), ('it', 'PRON'), ('shall', 'NN'), ('be', 'VERB'), ('an', 'DET'), ('holy', 'NN'), ('anointing', 'NN'), ('oil', 'NN'), ('.', '.')]\n",
      "\n",
      "\n",
      "sentence 2\n",
      "[('18', 'NN'), (':', '.'), ('18', 'NN'), ('As', 'ADP'), ('for', 'ADP'), ('his', 'DET'), ('father', 'NN'), (',', '.'), ('because', 'ADP'), ('he', 'PRON'), ('cruelly', 'NN'), ('oppressed', 'NN'), (',', '.'), ('spoiled', 'NN'), ('his', 'DET'), ('brother', 'NN'), ('by', 'ADP'), ('violence', 'NN'), (',', '.'), ('and', 'CONJ'), ('did', 'VERB'), ('that', 'ADP'), ('which', 'DET'), ('is', 'VERB'), ('not', 'ADV'), ('good', 'ADJ'), ('among', 'NN'), ('his', 'DET'), ('people', 'NOUN'), (',', '.'), ('lo', 'NN'), (',', '.'), ('even', 'ADV'), ('he', 'PRON'), ('shall', 'NN'), ('die', 'NN'), ('in', 'ADP'), ('his', 'DET'), ('iniquity', 'NN'), ('.', '.')]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "gutenberg_corpus = nltk.corpus.gutenberg\n",
    "bible = gutenberg_corpus.sents('bible-kjv.txt')\n",
    "tagged_bible = baseline_tagger.tag_sents(bible)\n",
    "\n",
    "for i in range(2):\n",
    "    print(f\"sentence {i+1}\")\n",
    "    randint = random.randint(0, len(tagged_bible))\n",
    "    print(tagged_bible[randint])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output:\n",
    "\n",
    "```\n",
    "sentence 1\n",
    "[('39', 'NN'), (':', '.'), ('23', 'NN'), ('The', 'DET'), ('quiver', 'NN'), ('rattleth', 'NN'), ('against', 'ADP'), ('him', 'PRON'), (',', '.'), ('the', 'DET'), ('glittering', 'NN'), ('spear', 'NN'), ('and', 'CONJ'), ('the', 'DET'), ('shield', 'NN'), ('.', '.')]\n",
    "\n",
    "sentence 2\n",
    "[('5', 'NN'), (':', '.'), ('11', 'NN'), ('And', 'CONJ'), ('the', 'DET'), ('children', 'NN'), ('of', 'ADP'), ('Gad', 'NN'), ('dwelt', 'NN'), ('over', 'ADP'), ('against', 'ADP'), ('them', 'PRON'), (',', '.'), ('in', 'ADP'), ('the', 'DET'), ('land', 'NN'), ('of', 'ADP'), ('Bashan', 'NN'), ('unto', 'NN'), ('Salcah', 'NN'), (':', '.'), ('5', 'NN'), (':', '.'), ('12', 'NN'), ('Joel', 'NN'), ('the', 'DET'), ('chief', 'NN'), (',', '.'), ('and', 'CONJ'), ('Shapham', 'NN'), ('the', 'DET'), ('next', 'NN'), (',', '.'), ('and', 'CONJ'), ('Jaanai', 'NN'), (',', '.'), ('and', 'CONJ'), ('Shaphat', 'NN'), ('in', 'ADP'), ('Bashan', 'NN'), ('.', '.')]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f) Experiment with different ratios and using only one dataset with a train/test split. Explain your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of permuatation 1: 0.052273\n",
      "Accuracy of permuatation 1: 0.052179\n",
      "Accuracy of permuatation 1: 0.052044\n",
      "Accuracy of permuatation 1: 0.052403\n"
     ]
    }
   ],
   "source": [
    "def get_train_test_split(dataset, train_split: float):\n",
    "    return split(dataset, train_size=train_split)\n",
    "\n",
    "tagger = taggers[1]\n",
    "dataset = brown_corpus.tagged_sents()\n",
    "\n",
    "train_sizes = [0.25, 0.5, 0.75, 0.9]\n",
    "\n",
    "\n",
    "for i in train_sizes:\n",
    "    print_accuracy_of_taggers([tagger], [split(dataset, train_size=i)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives the following output:\n",
    "\n",
    "```\n",
    "Accuracy: 0.052273\n",
    "Accuracy: 0.052179\n",
    "Accuracy: 0.052044\n",
    "Accuracy: 0.052403\n",
    "```\n",
    "\n",
    "As we can see the difference in accuracy does not change that much between the different ratios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Tagging with probabilities\n",
    "\n",
    "Hidden Makrov Models (HMMs) can be used to solve Part-of-Speech (POS) tagging. Use HMMs to calculate probabilities for words and tags, using the appended code.\n",
    "\n",
    "Implementation of the methods is found in a seperate file, and the output is found here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Implement the missing pieces of the function task3a() found in the appended code. Also found on the next page for reference.\n",
    "\n",
    "Implemented in seperate file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\".\")\n",
    "\n",
    "from lab2_task3_helper import lab2_helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Print the probability of...\n",
    "* a verb (VB) being \"run\"\n",
    "* a preposition (PP) beging followed by a verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prob. of a Verb(VB) being 'run' is 0.1329%\n",
      "Prob. of a Preposition(PP) being followed by a Verb(VB) is 25.1591%\n"
     ]
    }
   ],
   "source": [
    "lab2_helper.task3b()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "```\n",
    "Prob. of a Verb(VB) being 'run' is 0.1329%\n",
    "Prob. of a Preposition(PP) being followed by a Verb(VB) is 25.1591%\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Print the 10 most common words for each of the tags NN, VB, JJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagwords, tags = lab2_helper.task3a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target tag: NN\n",
      "[('time', 1555), ('man', 1148), ('Af', 994), ('years', 942), ('way', 883), ('people', 809), ('men', 736), ('world', 684), ('life', 676), ('year', 647)]\n",
      "Target tag: VB\n",
      "[('said', 1943), ('made', 1119), ('make', 765), ('see', 727), ('get', 719), ('know', 676), ('came', 621), ('used', 610), ('go', 604), ('come', 589)]\n",
      "Target tag: JJ\n",
      "[('new', 1060), ('such', 903), ('own', 750), ('good', 693), ('great', 592), ('New', 575), ('old', 568), ('American', 535), ('small', 517), ('long', 515)]\n"
     ]
    }
   ],
   "source": [
    "target_tags = [\"NN\", \"VB\", \"JJ\"]\n",
    "\n",
    "for target in target_tags:\n",
    "    print(f\"Target tag: {target}\")\n",
    "    print(tagwords[target].freqdist().most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Print the probability of the tag sequence PP VB VB DT NN for the sentence “I can code some code”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.171716745411479e-22\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.0%'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probability = tagwords[\"START\"].prob(\"START\") * tags[\"START\"].prob(\"PP\") * \\\n",
    "    tagwords[\"PP\"].prob(\"I\") *  tags[\"PP\"].prob(\"VB\") * \\\n",
    "    tagwords[\"VB\"].prob(\"can\") * tags[\"VB\"].prob(\"VB\") * \\\n",
    "    tagwords[\"VB\"].prob(\"code\") * tags[\"VB\"].prob(\"DT\") * \\\n",
    "    tagwords[\"DT\"].prob(\"some\") * tags[\"DT\"].prob(\"NN\") * \\\n",
    "    tagwords[\"NN\"].prob(\"code\") * \\\n",
    "    tagwords[\"END\"].prob(\"END\")\n",
    "\n",
    "print(probability)\n",
    "lab2_helper.prettify(probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I get a very low probability of 6.171716745411479e-22."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6b8f20c097e32b53f09a95007ec526e0eb5f6178dd7117c218d8f10eacb6c81e"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit ('nltk': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
