{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e3491f9-3412-4dea-8b7c-719b2f60b38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2dc3d7e-7671-4877-8a22-db25eadfe0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following can be of use when identifying tags:\n",
    "# nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65eafa3f-8101-4c3b-a7be-318b87f3a701",
   "metadata": {},
   "source": [
    "Let's use the familiar brown corpus to begin with. Get the POS-tagged sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd2772fc-6db0-4114-984f-6a076afca47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = nltk.corpus.brown.tagged_sents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cd7276-bce6-410b-8c4f-2ce703c08721",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Exercise 1 - Introduction to chunking\n",
    "\n",
    "## 1a)\n",
    "Make your own noun phrase (NP) chunker, detecting noun phrases and a clause, for which verbs (VB) are followed by a preposition (IN) and/or a noun phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10a60423-b211-4a95-a5be-2f6196d86698",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r'''\n",
    "NP: {<DT|JJ.*>*?<NN.*>+}\n",
    "CLAUSE: {<VB.*>+<IN|NP>+}\n",
    "'''\n",
    "\n",
    "chunk_parser = nltk.RegexpParser(grammar) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dfc6146-7adf-4f54-9ca1-77575b5cc425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,560.0,168.0\" width=\"560px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S</text></svg><svg width=\"7.14286%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">He</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">PPS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"3.57143%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"7.14286%\" x=\"7.14286%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">is</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">BEZ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"10.7143%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"7.14286%\" x=\"14.2857%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">not</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">*</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"17.8571%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"22.8571%\" x=\"21.4286%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">CLAUSE</text></svg><svg width=\"75%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">interested</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"37.5%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"25%\" x=\"75%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">in</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"87.5%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"32.8571%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"10%\" x=\"44.2857%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">being</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">BEG</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"49.2857%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"10%\" x=\"54.2857%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">named</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"59.2857%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"5.71429%\" x=\"64.2857%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">a</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">AT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"67.1429%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"30%\" x=\"70%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"52.381%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">full-time</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"26.1905%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"47.619%\" x=\"52.381%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">director</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"76.1905%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"85%\" y1=\"1.2em\" y2=\"3em\" /></svg>",
      "text/plain": [
       "Tree('S', [('He', 'PPS'), ('is', 'BEZ'), ('not', '*'), Tree('CLAUSE', [('interested', 'VBN'), ('in', 'IN')]), ('being', 'BEG'), ('named', 'VBN'), ('a', 'AT'), Tree('NP', [('full-time', 'JJ'), ('director', 'NN')])])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test your parser!\n",
    "test_sentence = sents[400][:10]  # just an example sentence, using the first 10 words\n",
    "chunks = chunk_parser.parse(test_sentence)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca0cd6c-017f-491e-9b69-6ea05f8fee5f",
   "metadata": {},
   "source": [
    "## 1b)\n",
    "Convert a POS tagged text to a list of tuples, where each tuple consists of a verb followed by a sequence of noun phrases and prepositions.\n",
    "Example: “the little cat sat on the mat” becomes (‘sat’, ‘on’, ‘NP’) . . . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffde0c89-6bfd-46f4-8a10-97eaec5d48b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks_to_verb_NP_tuples(tagged_sents):\n",
    "    tuples = set()\n",
    "    \"\"\"\n",
    "    iterate the trees and subtrees of your parser.\n",
    "    add all chunks starting with a verb (CLAUSE) to the set of tuples\n",
    "    \"\"\"\n",
    "    cp = nltk.RegexpParser(r'''\n",
    "    NP: {<DT|JJ.*>*?<NN.*>+}\n",
    "    target: {<VB.*><NP|IN>+}\n",
    "    ''')\n",
    "\n",
    "    for sent in tagged_sents:\n",
    "        tree = cp.parse(sent)\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == 'target':\n",
    "                tuples.add((subtree[0][0], subtree[1][0], \"NP\"))\n",
    "\n",
    "    return list(tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03e33825-e3cd-453d-8c96-473ff41d6bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('left', 'next', 'NP'),\n",
       " ('shifting', ('styles', 'NNS-HL'), 'NP'),\n",
       " ('hold', 'to', 'NP'),\n",
       " ('broadens', ('shadows', 'NNS'), 'NP'),\n",
       " ('checked', 'by', 'NP'),\n",
       " ('showed', ('small', 'JJ'), 'NP'),\n",
       " ('working', ('days', 'NNS'), 'NP'),\n",
       " ('buried', 'under', 'NP'),\n",
       " ('commencing', 'with', 'NP'),\n",
       " ('allowed', ('parent', 'NN-HL'), 'NP'),\n",
       " ('Listen', 'to', 'NP'),\n",
       " ('got', ('that', 'DT'), 'NP'),\n",
       " ('flailed', 'at', 'NP'),\n",
       " ('picked', 'Cathy', 'NP'),\n",
       " ('snows', ('peas', 'NNS'), 'NP'),\n",
       " ('buttressed', 'by', 'NP'),\n",
       " ('spiral', 'into', 'NP'),\n",
       " ('achieve', ('goals', 'NNS'), 'NP'),\n",
       " ('rising', ('tide', 'NN'), 'NP'),\n",
       " ('allow', ('vent', 'NN'), 'NP')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check your output :-)\n",
    "import random\n",
    "\n",
    "vb_np = chunks_to_verb_NP_tuples(sents)\n",
    "random.shuffle(vb_np)\n",
    "vb_np[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b1a67a-af47-4d70-8ae3-350aa2d426aa",
   "metadata": {},
   "source": [
    "## 1c)\n",
    "Using the pre-annotated test set from wall street journal data (conll2000 in nltk), experiment with different grammars to get the highest possible F-measure. There is no evaluation of this task, but rather a motivator to learn something about grammars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81813066-7db9-4e03-bae8-6977d0b96f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  79.9%%\n",
      "    Precision:     70.3%%\n",
      "    Recall:        61.0%%\n",
      "    F-Measure:     65.3%%\n"
     ]
    }
   ],
   "source": [
    "wsj = nltk.corpus.conll2000\n",
    "test_sents = wsj.chunked_sents('test.txt', chunk_types=['NP'])\n",
    "# {<DT>?<JJ.*>?<NN.*>+}\n",
    "# (\\w+ DT)? (\\w+ JJ)*|(\\w+ (?:NN|NP|PRN)\n",
    "\n",
    "chunk_parser = nltk.RegexpParser(r'''\n",
    "NP: {<DT>?<JJ.*>?<NN.*>+}\n",
    "''') #TODO: your parser\n",
    "print(chunk_parser.accuracy(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5665d0-484e-41db-8d0e-27bb4bb83b72",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Exercise 2 - Making use of chunks\n",
    "\n",
    "## 2a)\n",
    "With the following grammar rules:\n",
    "```\n",
    "1. proper noun singular\n",
    "2. determiner followed by an adjective, followed by any noun\n",
    "3. two consecutive nouns\n",
    "```\n",
    "Create a `RegexpParser` chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b63178b-904f-476a-a1e7-8f80a69d0264",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r'''\n",
    "1: {<NNP>}\n",
    "2: {<DT><JJ><NN.*>}\n",
    "3: {<NN.*><NN.*>}\n",
    "'''\n",
    "# TODO: set up a parser using the grammar you defined\n",
    "chunk_parser = nltk.RegexpParser(grammar)\n",
    "\n",
    "def chunker(sent):\n",
    "    return chunk_parser.parse(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af10b7c7-f39d-4758-85b5-648a30e2e867",
   "metadata": {},
   "source": [
    "## 2b)\n",
    "\n",
    "Read the file `starlink.txt` and perform the following operations on the text:\n",
    "- sentence tokenize\n",
    "- word tokenize\n",
    "- pos tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd2f1666-0e02-46e0-bef8-5022638ec9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\"\"\"\n",
    "TODO:\n",
    "- read the file \"starlink.txt\"\n",
    "- tokenize and tag the words of each sentence\n",
    "\n",
    "the below function is just an idea of generalization to any file.\n",
    "you can delete this and write your own code.\n",
    "\"\"\"\n",
    "def get_pos_tags_from_file(filename):\n",
    "    cwd = os.getcwd()\n",
    "    data_dir = os.path.join(cwd, \"data\")\n",
    "    with open(os.path.join(data_dir, filename), \"r\") as f:\n",
    "        working_file = f.read()\n",
    "    sents = nltk.sent_tokenize(working_file)\n",
    "    words = [nltk.word_tokenize(sent) for sent in sents]\n",
    "    pos_words = nltk.pos_tag_sents(words)\n",
    "    \n",
    "    return pos_words\n",
    "    \n",
    "starlink_tagged = get_pos_tags_from_file(\"starlink.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3981bdb-550e-4ed6-bf48-d3ad3222765b",
   "metadata": {},
   "source": [
    "## 2c)\n",
    "From all found subtrees in the text, print out the text from all the leaves on the form of `DT -> JJ -> NN` (i.e. the CONSECUTIVE chunk you defined above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cae2440-cf9a-4e22-8642-34463366d871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a one-day delay',\n",
       " 'a successful deployment',\n",
       " 'the other launches',\n",
       " 'a classified payload',\n",
       " 'the first launch',\n",
       " 'a solar storm',\n",
       " 'a different approach',\n",
       " 'The previous launch',\n",
       " 'a single burn',\n",
       " 'the upper stage',\n",
       " 'a second burn',\n",
       " 'the atmospheric drag',\n",
       " 'the previous set']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "TODO:\n",
    "write a function to retrieve the DT-JJ-NN sequences\n",
    "from the grammar you defined in 2a)\n",
    "\"\"\"\n",
    "def get_descriptive_nouns(tagged_sents):\n",
    "    descriptive_nouns = []\n",
    "\n",
    "    for sent in tagged_sents:\n",
    "        for subtree in chunker(sent).subtrees():\n",
    "            if subtree.label() == \"2\":\n",
    "                text = (\" \".join([w for w, POS in subtree.leaves()]))\n",
    "                descriptive_nouns.append(text)\n",
    "    return descriptive_nouns\n",
    "\n",
    "\n",
    "get_descriptive_nouns(starlink_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16703648-ada4-400b-943a-a4783b6b3cfd",
   "metadata": {},
   "source": [
    "## 2d)\n",
    "Create a custom rule for a combination of 3 or more tags, similarly to task c).\n",
    "\n",
    "Do you see any practical uses for chunking with this rule, or in general?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376134e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3893d4dd-5b8a-4fe5-aa44-d2212ef3c060",
   "metadata": {},
   "source": [
    "# Exercise 3 - Context-free grammar (CFG)\n",
    "\n",
    "## 3a)\n",
    "Create a cfg to handle sentences such as \"she is programming\", \"they are coding\" (look at the word forms and POS). The first verb should only handle present tense, while the second verb is flexible. Note that you need to specify the accepted words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745fb95d-f9a0-4df5-8c58-7e5f3b89de2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = nltk.CFG.fromstring(\"\"\"\n",
    "TODO\n",
    "\"\"\")\n",
    "cfg.productions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a421b27b-a9f0-4786-b255-7a188698ee4d",
   "metadata": {},
   "source": [
    "#### A little function to visualize some possible outputs of the grammar\n",
    "#### Do not change this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980a4a86-e97e-46ee-84b5-054012b7bb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(grammar, start, tokens):        \n",
    "    # iterate left hand and right hand side of the tree\n",
    "    if start in grammar._lhs_index:\n",
    "        derivation = random.choice(grammar._lhs_index[start])            \n",
    "        for rhs in derivation._rhs:          \n",
    "            generate_sample(grammar, rhs, tokens)\n",
    "    elif start in grammar._rhs_index:\n",
    "        tokens.append(str(start))\n",
    "    return tokens\n",
    "\n",
    "for _ in range(10):\n",
    "    print(generate_sample(cfg, cfg.start(), []))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5ca435-63fe-494e-8112-52160f4e7dc8",
   "metadata": {},
   "source": [
    "## 3b)\n",
    "Find some problems with the above CFG, any ideas how you would improve the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8890f79c-1f7a-4f57-9730-46c1cd0f894d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3c)\n",
    "Initialize a `ChartParser` with the cfg from 3a).\n",
    "\n",
    "Write a function to retrieve words not defined by your grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a783e61-4b0b-4a20-b1e4-6c919fce2a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_parser = None # TODO chartparser of cfg grammar above\n",
    "\n",
    "\"\"\"\n",
    "TODO\n",
    "write a function that returns a list of missing tokens (not covered by your cfg)\n",
    "Look up \"lexical index\" of a grammar.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695cacfa-6208-4e6b-b660-5f7579d9e134",
   "metadata": {},
   "source": [
    "Finish the below function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06c4cac-0866-4b0d-b48d-694febd2e7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(parser, cfg_grammar, sent):\n",
    "    tokens = None # TODO word tokens\n",
    "    missing = get_missing_words(cfg_grammar, tokens)\n",
    "    if missing:\n",
    "        print(\"Grammar does not cover: {}\".format(missing))\n",
    "        return\n",
    "    trees = None # TODO: a list of parsed tokens\n",
    "    for tree in trees:\n",
    "        print(tree)\n",
    "    if len(trees) > 0:\n",
    "        return trees[0]\n",
    "    else:\n",
    "        print(\"Ungrammatical sentence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d27bda0-b0e8-41af-ab0a-2806cce77911",
   "metadata": {},
   "source": [
    "## 3d)\n",
    "output the tree of your parser for the sentence \"they are programming\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4050a004-7c92-45cb-94f9-0eea873e35b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"they are programming\"\n",
    "parse(cfg_parser, cfg, txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46647cf-3e1c-426d-a84f-ca21d20ac632",
   "metadata": {},
   "source": [
    "# Exercise 4 - Tweet like Trump! Now that he's banned\n",
    "Using the provided file \"realDonaldTrump.json\", you will build a language model to generate Trump-esque tweets using n-grams.\n",
    "\n",
    "Hint: make use of \"padded_everygram_pipeline\" supported in nltk.lm. This creates all ngrams up to the specified N-parameter with padding:\n",
    "\n",
    "Example:\n",
    "```\n",
    "('<s>',)\n",
    "('<s>', '<s>')\n",
    "('<s>', '<s>', '<s>')\n",
    "('<s>', '<s>', '<s>', '<s>')\n",
    "('<s>', '<s>', '<s>', '<s>', 'i')\n",
    "('<s>',)\n",
    "('<s>', '<s>')\n",
    "('<s>', '<s>', '<s>')\n",
    "('<s>', '<s>', '<s>', 'i')\n",
    "('<s>', '<s>', '<s>', 'i', 'am')\n",
    "('<s>',)\n",
    "('<s>', '<s>')\n",
    "('<s>', '<s>', 'i')\n",
    "('<s>', '<s>', 'i', 'am')\n",
    "('<s>', '<s>', 'i', 'am', 'asking')\n",
    "('<s>',)\n",
    "('<s>', 'i')\n",
    "('<s>', 'i', 'am')\n",
    "('<s>', 'i', 'am', 'asking')\n",
    "('<s>', 'i', 'am', 'asking', 'for')\n",
    "('i',)\n",
    "('i', 'am')\n",
    "('i', 'am', 'asking')\n",
    "('i', 'am', 'asking', 'for')\n",
    "('i', 'am', 'asking', 'for', 'everyone')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425e851b-4bdf-4ba3-936c-bf75abd19cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import os\n",
    "# TODO imports for nltk n-gram modeling and LM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6d8349-1785-4fc9-81f5-bebc39bbe7d8",
   "metadata": {},
   "source": [
    "Load the JSON data and store the texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759c0cd1-ae87-4dd3-9b3c-073e57b7a878",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/realDonaldTrump.json\") as fp:\n",
    "    tweets = json.load(fp)\n",
    "\n",
    "texts = list(map(lambda x: x.get(\"text\"), tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56baa803-f19c-40fa-86de-ac3c4bfc2e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use this path to store our model after training, so it's easier to reuse later :)\n",
    "pickle_path = \"model.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8543459-3dbd-498a-bc2f-37a1ec19e891",
   "metadata": {},
   "source": [
    "Finish the `train_model` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519e072b-bf32-4c2b-9e33-1807a3b827df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data):\n",
    "    tokenized = None  # TODO: use a tokenizer for the twitter data\n",
    "    n = 0 # TODO find an appropriate N-gram \n",
    "    train_data, padded_vocab = None # TODO: padded everygram\n",
    "    # \n",
    "    model = None # TODO: from nltk.lm (language model), use an appropriate estimator\n",
    "    model.fit(train_data, padded_vocab)\n",
    "    # save the model if you want to :-) then we can load it in the next step without retraining!\n",
    "    with open(pickle_path, \"wb\") as fp:\n",
    "        pickle.dump(model, fp)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffef0f5-ad85-4563-828c-09675426a00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(pickle_path):\n",
    "    with open(pickle_path, \"rb\") as fp:\n",
    "        model = pickle.load(fp)\n",
    "else:\n",
    "    model = train_model(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921605f1-8d98-4970-958a-a72ca9601e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(model, txt):\n",
    "    txt = None  # TODO: tokenize the input\n",
    "    while True:\n",
    "        next_word = model.generate(text_seed=txt, random_seed=42)\n",
    "        if next_word == '</s>':\n",
    "            break\n",
    "        txt.append(next_word)\n",
    "        \n",
    "    def filter_fn(txt):\n",
    "        no_http = \"http\" not in txt\n",
    "        some_other_rule = True\n",
    "        \n",
    "        return no_http or some_other_rule\n",
    "    \n",
    "    return \" \".join([t for t in txt if filter_fn(t)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5a6b67-2c8b-4d91-99b8-f229096e42da",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_sentence(model, \"some sentence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a7942b-613c-4a1e-8e69-1e3167f41a66",
   "metadata": {},
   "source": [
    "## 4b)\n",
    "Create a grammar to chunk some typical trump statements.\n",
    "\n",
    "There are multiple approaches to this. One way would be to use your own input to the model and look at the resulting outputs and their POS tags. Another possible approach is to use the training data to group together e.g. 5-grams of POS tags to look at the most frequently occurring POS tag groupings. The aim is to have a chunker that groups utterances like \"so sad\", \"make america great again!\" and so forth.\n",
    "\n",
    "Show your results using the outputs from your model with these inputs: \n",
    "- \"clinton will\"\n",
    "- \"obama is\"\n",
    "- \"build a\"\n",
    "- \"so sad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bab1343-f28b-49c8-a499-617012cf3398",
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_grammar = r\"\"\"\n",
    "TODO\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6b8f20c097e32b53f09a95007ec526e0eb5f6178dd7117c218d8f10eacb6c81e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
